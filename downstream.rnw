%\VignetteIndexEntry{BeadArrayUseCases Vignette}
%\VignetteDepends{}
%\VignetteKeywords{Illumina Microarray Expression}
%\VignettePackage{BeadArrayUseCases}

\documentclass[a4paper,11pt]{article}

\textwidth=6.2in
\textheight=8.5in
\oddsidemargin=.1in
\evensidemargin=.1in
\headheight=-.3in
\parindent=0pt


\usepackage{amsthm,ragged2e,marvosym,wasysym}
\usepackage{mls40Sweave}
\usepackage[utf8]{inputenc}
\usepackage{sidecap}

\newcommand{\Rfunction}[1]{{\texttt{#1}}}
\newcommand{\Robject}[1]{{\texttt{#1}}}
\newcommand{\Rpackage}[1]{{\textsf{#1}}}
\newcommand{\Rmethod}[1]{{\texttt{#1}}}
\newcommand{\Rfunarg}[1]{{\texttt{#1}}}
\newcommand{\Rclass}[1]{{\textit{#1}}}

\SweaveOpts{eval=FALSE, keep.source=FALSE, results=hide}

\title{Further analysis of microarray data in Bioconductor}

\author{Mark Dunning}


\renewcommand\labelenumi{\textbf{Exercise \theenumi}}


% Sweave("IlluminaPLoSVignette.rnw")
\newtheoremstyle{labexc}%
{9pt}{12pt}%      space above and below
{\sffamily\RaggedRight}%              body style
{0pt}%       heading indent amount
{\sffamily\bfseries}{:}% heading font and punctuation after it
{ }%         space after heading is a new line
{}%          head spec (empty = same as 'plain' style)

\newtheoremstyle{myplain}%
{9pt}{9pt}%      space above and below
{\RaggedRight}%              body style
{0pt}%       heading indent amount
{\sffamily\bfseries}{}% heading font and punctuation after it
{ }%         space after heading is a new line
{}%          head spec (empty = same as 'plain' style)

\newtheoremstyle{mywarning}%
{9pt}{9pt}%      space above and below
{\itshape\RaggedRight}%              body style
{0pt}%       heading indent amount
{\sffamily\bfseries}{}% heading font and punctuation after it
{ }%         space after heading is a new line
{}%          head spec (empty = same as 'plain' style)

\theoremstyle{myplain} \newtheorem*{textinfo}{\large\Info}
\theoremstyle{labexc} \newtheorem*{exc}{Use Case}
\theoremstyle{mywarning} \newtheorem*{notebell}{\Large\bell}
\theoremstyle{myplain} \newtheorem*{noteright}{\Large\Pointinghand}
\theoremstyle{myplain} \newtheorem*{textstop}{\Large\Stopsign}
\theoremstyle{myplain} \newtheorem*{commentary}{\Large\CheckedBox}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}


\maketitle
\textinfo{Previously in this course, we have presented examples of how to pre-process gene-expression data generated on a specific platform (Illumina or Affymetrix) and produce a list of genes that show statistical evidence for differential expression. In most modern microarray experiments this is rarely the end-point of the analysis, and here we will introduce other ways that we can interrogate our gene expression data, which can be more open-ended or exploratory. The methods that we will apply rely on the data being stored in a common format such as the \Rclass{ExpressionSet} class using in Bioconductor. Hence, we can analyse any microarray technology using the same steps. Indeed, the data doesn't neccesarily need to be expression data. 
}

<<setWidth, eval=TRUE, echo=FALSE>>=
options(width=70);
@


We will use example datasets from Gene Expression Omnibus and a Bioconductor data package. When importing such datasets, it is important to check the properties of the data to ensure that the distributional assumptions of our methods are valid, and that the appropriate sample meta data are available to allow comparisons to be made. 

\begin{itemize}
\item{Have the data been normalized? If so, what method was used?}
\item{Has quality control been performed?}
\item{Is there sufficient sample meta-data to perform the analysis?}
\item{Are there any batch-effects remaining in the data?}
\end{itemize}

\textinfo{Section 1 will illustrates unsupervised exploratory analysis techniques and also a Gene Ontology analysis. Section 2 will use  a supervised approach to find a classifier for breast cancer samples.}

\section{Colon cancer data from GEO}

We will consider GSE33126, which comprises nine paired tumor/normal colon tissues on Illumina HT12\_v3 gene expression Beadchips. These data were generated to inform a comparison of technologies for microRNA profiling. However, we will only use the mRNA data here. 

You should already have the raw GEO data on your computer in the {\tt Downstream} folder. If not, it can be downloaded from the GEO website and then imported into R using {\tt GEOquery}.
\begin{exc}
Download the dataset with GEO ID GSE33126 and import the data using the \Rpackage{GEOquery} package.
\end{exc}

<<>>=
library(GEOquery)
url <- "ftp://ftp.ncbi.nih.gov/pub/geo/DATA/SeriesMatrix/GSE33126/"
filenm <- "GSE33126_series_matrix.txt.gz"
if(!file.exists("GSE33126_series_matrix.txt.gz")) download.file(paste(url, filenm, sep=""), destfile=filenm)
colonData <- getGEO(filename=filenm)
@


We now need to explore the data to ensure that they have been normalised and are on an appropriate scale for analysis.

\begin{exc}
Are the data on the log$_2$ scale? Are they normalised? Look at the phenotypic data stored with the object and find information about the sample groups in the study and patient IDs.
\end{exc}

<<>>=
head(exprs(colonData))
exprs(colonData) <- log2(exprs(colonData))
boxplot(exprs(colonData),outline=FALSE)
pData(colonData)[1:2,]

SampleGroup <- pData(colonData)$source_name_ch1
Patient <- pData(colonData)$characteristics_ch1.1
@



\textinfo{The \Rpackage{ArrayExpress} package can be used in a similar way to download data from the ArrayExpress repository}


\subsection{Filtering the data}

A first step in the analysis of microarray data is often to remove any uniformative probes. We can do this because typically only 50$\%$ of probes genes will be expressed, and even fewer than this will be differentially expressed. Including such non-informative genes in visualisation will obscure any biological differences that exist. The \Rpackage{genefilter} package contains a suite of tools for the filtering of microarray data. The \Rfunction{varFilter} function allows probes with low-variance to be removed from the dataset. The metric using to decide which probes to remove is the Inter-Quartile Range (\textit{IQR}), and by default half of the probes are removed. Both the function used to do the filtering, and cut-off can be specified by the user.

\begin{exc}
Remove probes with low-variance from the colon cancer dataset.
\end{exc}

<<genefilter>>=
library(genefilter)
dim(colonData)
varFiltered <- varFilter(colonData)
dim(varFiltered)
nrow(colonData) / nrow(varFiltered)
@


\subsection{Calculating a distance matrix}
The first step towards clustering the data is to construct a \textit{distance matrix}. Each entry in this matrix is the pairwise distance between two samples. Note that for expression data we have to transpose the standard ExpressionSet format, as clustering is designed to work on rows rather than columns. The standard function to make a distance matrix in R is \Rfunction{dist} which uses the \textit{euclidean} metric. As the data entries in a distance matrix are symmetrical, it has a different representation to the default matrix (i.e. values are not repeated unnecessarily), and clustering algorithms are able to accept this representation as input.

\begin{exc}
Construct a matrix of Euclidean distances for the variance-filtered dataset
\end{exc}

<<distanceMatrix>>=
euc.dist <- dist(t(exprs(varFiltered)))
euc.dist
@
For gene-expression data, it is common to use correlation as a distance metric rather than the Euclidean. \textit{You should make sure that you know the difference between the two metrics}. The \Rfunction{cor} function can be used to calculate the correlation of columns in a matrix. Each row (or column) in the resulting matrix is the correlation of that sample with all other samples in the dataset. The matrix is symmetrical and we can transform this into a distance matrix by first subtracting 1 from the correlation matrix. Hence, samples with a higher correlation have a smaller 'distance'. 

\begin{exc}
Calculate a matrix of sample correlations and transform into a distance matrix
\end{exc}

<<>>=
corMat <- cor(exprs(varFiltered))
corMat
cor.dist <- as.dist(1-corMat) 
@
\textinfo{The default method used to calculate correlations is the 'Pearson' method. If you wish to use the 'Spearman' (non-parametric) method you can specify {\tt method="spearman"} in the call to \Rfunction{cor}.}

\subsection{Hierachical clustering}

Hierachical clustering is the method by which samples with similar profiles are grouped together, based on their distances. The method for grouping samples can be specified, with the default being to use \textit{complete linkage}. Different linkage methods can affect the properties of the clustering output. e.g. the 'ward' method tends to produce smaller, compact clusters. A popular way of displaying the results of clustering is a \textit{dendrogram}, which arranges the samples on the x-axis and shows the distances between samples on the y-axis.
It is important to note that the distance of samples on the x-axis has no meaning. The fact that two samples are displayed next to each other, does not mean that they are closest together. One has to use the y-axis to determine their distance

\begin{exc}
Perform a hierarchical clustering on the filtered data using Euclidean distances. Are there any distinct groups in the data? Try using the correlation based similarity measure and complete linkage. Does the choice of method effect your clustering?
\end{exc}

<<clustering>>=
par(mfrow=c(1,2))
clust.euclid = hclust(euc.dist)
clust.cor = hclust(cor.dist)

par(mfrow=c(1,2))
plot(clust.euclid,label = SampleGroup)
plot(clust.cor,label = SampleGroup)

@


\begin{exc}
Experiment with different methods of building the clustering; such as 'ward'.
\end{exc}
<<>>=
plot(hclust(euc.dist,method="ward"))
@


\subsection{Extracting data from the clustering}

If we want to interpret the data presented in a clustering analysis, we need a way of extracting which samples are grouped together, or to determine the optimal grouping of samples. One intuitive way of assigning groups it to 'cut' the dendrogram at a particular height on the y-axis. We can do this manually on the plot, or use the \Rfunction{cutree} function to return the labels of samples that are belong to the same group when the dendrogram is cut at the specified height, \textit{h}. Alternatively, we can specify how many groups, \textit{k}, that we want to create.

\begin{exc}
'Cut' the dendrogram at a height of 0.12. How many groups are formed? Now cut the dendrogram to obtain three groups. How well do these groups agree with the original sample groupings?
\end{exc}

<<>>=
library(cluster)
par(mfrow=c(1,1))
plot(clust.cor)
abline(h=0.12,col="red")
cutree(clust.cor, h=0.12)

cutree(clust.cor, k = 2)
table(cutree(clust.cor, k = 3),SampleGroup)

@

A \textit{Silhouette plot} can be used to choose the optimal number of clusters. For each sample, we calculate a value that quantifies how well it 'fits' the cluster that it has been assigned to. If the value is around 1, then the sample closely fits other samples in the same cluster. However, if the value is around 0 the sample could belong to another cluster. In the silhouette plot, the values for each cluster are plotted together and ordered from largest to smallest. The number of samples belonging to each group is also displayed. 

\begin{exc}
Produce silhouette plots for the result of cutting the dendrogram to give 2,3,4 and 5 clusters. Which value of {\tt k} seems to be the most sensible?
\end{exc}

<<>>=
par(mfrow=c(2,2))
plot(silhouette(cutree(clust.cor, k = 2), cor.dist), col = "red", main = paste("k=", 2))
plot(silhouette(cutree(clust.cor, k = 3), cor.dist), col = "red", main = paste("k=", 3))
plot(silhouette(cutree(clust.cor, k = 4), cor.dist), col = "red", main = paste("k=", 4))
plot(silhouette(cutree(clust.cor, k = 5), cor.dist), col = "red", main = paste("k=", 5))
@

If we have prior knowledge of how many clusters to expect, we could run the clustering in a supervised manner. The \textbf{P}artition \textbf{A}round \textbf{M}edioids method can be used to group samples into \textit{k} clusters.

\begin{exc}
Group the samples into two clusters using the \Rfunction{pam} method
\end{exc}
<<>>=
library(cluster)
pam.clus <- pam(euc.dist,k=2)
clusplot(pam.clus)
pam.clus$clustering
table(pam.clus$clustering,SampleGroup)
@



\subsection{Producing a heatmap}

A heatmap is often used to visualise differences between samples. Each row represents a
gene and each column is an array and coloured cells indicate the expression levels of genes. Both
samples and genes with similar expression profile are clustered together. By default, euclidean distances are used with complete linkage clustering.

Drawing a heatmap in R uses a lot of memory and can take a long time, therefore reducing
the amount of data to be plotted is usually recommended. Including too many non-informative genes can also make it difficult to spot patterns. Typically, data are filtered to include the genes which tell us the most about the biological variation. In an \textit{un-supervised} setting, the selection of such genes is done without using prior knowledge about the sample groupings.


\begin{exc}
Make a heatmap using the 100 most variable genes in the experiment according to
their inter-quartile range (IQR). Can you see any structure in the data?
\end{exc}
<<heatmap>>=
IQRs = apply(exprs(varFiltered), 1, IQR)
highVarGenes = order(IQRs,decreasing=T)[1:100]
Symbols <- as.character(fData(colonData)$Symbol[highVarGenes])
heatmap(as.matrix(exprs(varFiltered)[highVarGenes, ]), labCol=SampleGroup,labRow=Symbols)
@

The default options for the heatmap are to cluster both the genes (rows) and samples (columns). However, sometimes we might want to specify a particular order. For example, we might want to order the columns according to sample groups. We can do this by re-ordering the input matrix manually and setting the \Rfunarg{Colv} to {\tt NA}. This tells the \Rfunction{heatmap} function not be cluster the columns. It choosing this option, we need to be careful that any column colourings or labels are set to the same ordering. Alternatively, a pre-calculated dendrogram could be used.

\begin{exc}
Produce a new heatmap with columns ordered according to the tumour / normal status.
\end{exc}

<<>>=
colOrd <- order(SampleGroup)
colOrd
heatmap(as.matrix(exprs(varFiltered)[highVarGenes, colOrd]),Colv=NA,labCol=SampleGroup[colOrd])
@

\begin{exc}
Use the 'ward' method to cluster the samples and pass this to the heatmap function
\end{exc}

<<>>=
clus.ward <- hclust(cor.dist,method="ward")
heatmap(as.matrix(exprs(varFiltered)[highVarGenes, ]),Colv=as.dendrogram(clus.ward),labCol=SampleGroup)

@


\subsection{Customising the heatmap}

The heatmap function can be customised in many ways to make the output more informative. For example, the \Rfunarg{labRow} and \Rfunarg{labCol} parameters can be used to give labels to the rows (genes) and columns (sample) of the heatmap. Similarly, \Rfunarg{ColSideColors} and \Rfunarg{RowSideColors} give coloured labels, often used to indicate different groups which are know in advance. See the help page for heatmap for more details.

\begin{exc}
Define a colour label to distinguish tumour and normal samples.
\end{exc}
<<>>=
labs <- as.factor(SampleGroup)
levels(labs) <- c("yellow", "blue")
heatmap(as.matrix(exprs(varFiltered)[highVarGenes, ]), labCol=Patient,ColSideColors=as.character(labs),labRow=Symbols)

@


The colours used to display the gene expression values can also be modified. For this, we can use the \Rpackage{RColorBrewer} package which has functions for creating pre-defined palettes. The function \Rfunction{display.brewer.all} can be used to display the palettes available through this package.

\begin{exc}
Change the colours to a red/blue scale
\end{exc}
<<>>=
library(RColorBrewer)
display.brewer.all()
hmcol<-brewer.pal(11,"RdBu")
heatmap(as.matrix(exprs(varFiltered)[highVarGenes, ]), ColSideColors=as.character(labs),labRow=Symbols,col=hmcol)
@

\textinfo{You should avoid using the traditional red / green colour scheme as it may be difficult for people with colour-blindness to interpret}


A popular use for heatmaps is to take an existing gene list (e.g. genes found to be significant in a previous study, or genes belonging to a particular pathway) and produce an image of how they cluster the data for exploratory purposes. This can be achieved easily by selecting the correct rows in the data matrix. 

\begin{exc}
Produce a heatmap of all genes belonging to the cell-cycle pathway (KEGG 04110)
\end{exc}

<<>>=
library(illuminaHumanv3.db)
pathwayGenes <- unlist(mget("04110", revmap(illuminaHumanv3PATH)))
pathwayGenes <- pathwayGenes[pathwayGenes %in% featureNames(varFiltered)]
symbols <- fData(varFiltered)[pathwayGenes,"Symbol"]
heatmap(as.matrix(exprs(varFiltered)[pathwayGenes, ]), ColSideColors=as.character(labs),labCol=Patient,labRow=symbols,col=hmcol)


@

\subsection{Principal Components Analysis}
Principal components analysis (PCA) is a data reduction technique that allows us to simplify multidimensional data sets to 2 or 3 dimensions for plotting purposes and identify which factors explain the most variability in the data. We can use the \Rfunction{prcomp} function to perform a PCA and we have to supply it with a distance matrix. The resulting object contains information on the proportion of variance that is 'explained' by each component. Ideally, we want to see that the majority of the variance is explained by the first 2 or 3 components, and that these components are associated with a biological factor

\begin{exc}
Perform a PCA on the correlation distances. How much variation is explained by the first two components?
\end{exc}

<<PCA>>=
pca <- prcomp(cor.dist)
plot(pca)
summary(pca)
@


The \Rpackage{ggplot2} package is convenient for plotting principal components results as it allows many different covariates to be overlaid on the plot. See {\tt http://ggplot2.org/} for more information on this package.
\begin{exc}
Display the first two components using ggplot2 and overlay the SampleGroup and cluster groupings that you obtained previously. Can you separate the tumours and normals on the plot?
\end{exc}

<<PCAplot>>=
library(ggplot2)

clusLabs <- cutree(clust.cor, k = 3)

pcRes <- data.frame(pca$rotation,SampleGroup,Sample = rownames(pca$x),Patient)

ggplot(pcRes, aes(x=PC1,y=PC2,col=SampleGroup,label=Patient,pch=as.factor(clusLabs))) + geom_point() + geom_text(vjust=0,alpha=0.5)

@

\begin{exc}
Compare the values of the first and second components for tumours and normals. Do they support the separation of sample groups?
\end{exc}

<<loadings>>=
ggplot(pcRes, aes(x=SampleGroup,y=PC1,fill=SampleGroup)) + geom_boxplot()
ggplot(pcRes, aes(x=SampleGroup,y=PC2,fill=SampleGroup)) + geom_boxplot()
@

\subsection{Gene-Ontology analysis}
In this section we give an example of how to find a list of relevant pathways / GO terms from a list of differentially-expressed genes. We will use the colon cancer data that we downloaded from GEO.

\subsubsection{Non-specific filtering}
We are now going to create a gene universe by removing genes for will not contribute to
the subsequent analysis. Such filtering is done without regarding the phenotype variables -
hence a ”non-specific” filter.
An Illumina Human6 chip contains around 48,00 probes, but less than half of these have enough
detailed information to useful for a GO analysis. Therefore we restrict the dataset to only
probes for which we have a Entrez ID.
It is also recommended to select probes with sufficient variability across samples to be interesting; as probes with little variability will no be interesting to the question we are trying
to answer. The interquartile-range of each probe across all arrays is commonly used for this
with a cut-off of 0.5.

\begin{exc}
Create the gene universe of all genes with Entrez ID and with sufficient variation
across samples. How big is the universe?
\end{exc}

<<>>=
library(illuminaHumanv3.db)
entrezIds = mget(rownames(exprs(colonData)), illuminaHumanv3ENTREZID, ifnotfound = NA)
haveEntrezId = names(entrezIds)[sapply(entrezIds, function(x) !is.na(x))]
entrezSubset = exprs(colonData)[haveEntrezId, ]
entrezIQR = apply(entrezSubset, 1, IQR)
selected = entrezIQR > 0.5
nsFiltered = entrezSubset[selected, ]
universeIds = unlist(mget(rownames(nsFiltered), illuminaHumanv3ENTREZID, ifnotfound = NA))
@

Remember that the size of the universe can have an effect on the analysis. If the universe
is made artificially large by including too many uninformative probes, the p-values for the
GO terms will appear more significant.


\subsubsection{Selecting genes of interest and performing Hypergeometric test}

We now test the genes in the universe to see which ones have significant differences between the two groups. For this, we use the \Rfunction{rowttests} function implemented in the \Rpackage{genefilter} package, which performs a t-test for each row with respect to a factor. The p-values of the
test can be extracted, with one p-value given for each probe.

\begin{exc}
Do a t-test for each probe between the two groups of samples we have identified.
How many probes are significant at the 0.05 level? Define a list of genes to be used in the
hypergeometric test by finding the Entrez Ids for these significant probes.
\end{exc}

<<>>=
library(GOstats)
library(genefilter)
fac = as.factor(SampleGroup)
ttests = rowttests(as.matrix(nsFiltered), fac)
smPV = ttests$p.value < 0.05
pvalFiltered = nsFiltered[smPV, ]
dim(pvalFiltered)
selectedEntrezIds = unlist(mget(rownames(pvalFiltered), illuminaHumanv3ENTREZID, ifnotfound = NA))
@

The hyperGTest function is used to do the hypergeometric test for GO terms. Rather
than passing a long list of parameters to the function. An object of type \Rclass{GOHyperGParams}
is created to hold all the parameters we need to run the hypergeometric test. This object
can then be passed to hyperGTest multiple times without having to re-type the parameters
each time.
The meanings of these parameters are as follows:
\begin{itemize}
\item geneIds - The list of identifiers for the genes that we have selected as interesting
\item universeGeneIds - The list of identifiers resulting from non-specific filtering
\item annotation - The name of the annotation package that will be used
\item ontology - The name of the GO ontology that will be tested; either BP, CC or MF
\item pvaluecutoff - p-value that we will use to select significant GO terms
\item testDirection - Either ”over” or ”under” for over or under represented terms respectively
\item conditional - A more sophisticated form of hypergeometric test, which takes the rela-
tionships between terms in the GO graph can be used if this is set to TRUE. For this
practical we will keep conditional = FALSE
\end{itemize}

\begin{exc}
Do a hypergeometric test to find which GO terms are over-represented in the
filtered list of genes. How many GO terms are significant with a p-value of 0.05?
\end{exc}

<<>>=
params = new("GOHyperGParams", geneIds = selectedEntrezIds, universeGeneIds = universeIds, annotation="illuminaHumanv3",ontology = "BP", pvalueCutoff = 0.05, conditional = FALSE, testDirection = "over")
hgOver = hyperGTest(params)
hgOver
@

The summary function can be used to view the results of the test in matrix form. The
rows of the matrix are arranged in order of significance. The p-value is shown for each GO
term along with with total number of genes for that GO term, number of genes we would be
expect to appear in the gene list by chance and that number that were observed. A descriptive name is also given for each term. The results can also be printed out to a HTML report
using htmlReport.

\begin{exc}
View the results of the top 20 GO terms and create a HTML report.
\end{exc}
<<>>=
summary(hgOver)[1:20,]
@

GOstats also has the facility to test for KEGG pathways and chromosome bands which
are over-reprsented. The procedure of creating a gene universe and set of selected genes is
the same. However, we have to use a different object for the parameters, as not all 
\begin{exc}
Repeat the hypergeometric test for chromsome bands and KEGG pathways
\end{exc}
<<>>=
keggParams = new("KEGGHyperGParams", geneIds = selectedEntrezIds, universeGeneIds = universeIds,
annotation = "illuminaHumanv3", pvalueCutoff = 0.05, testDirection = "over")

keggHgOver = hyperGTest(keggParams)
summary(keggHgOver)




chrParams = new("ChrMapHyperGParams", geneIds = selectedEntrezIds, universeGeneIds = universeIds,annotation = "illuminaHumanv3", pvalueCutoff = 0.05, testDirection = "over",conditional = TRUE)
chrHgOver = hyperGTest(chrParams)
summary(chrHgOver)
@

\subsection{A one-off test}
Ocassionally, we might want to check if a particular set of genes appear to be up- or down-regulated in an analysis. This can be checked using the \Rfunction{geneSetTest} function in \Rpackage{limma}, which computes a p-value to test the hypothesis that the selected genes have more extreme test-statistics than one might expect by chance. Moreover, separate tests can be performed to see if the selected genes are up-regulated ({\tt alternative=up}) or down-regulated ({\tt alternative=down}). The default is to test for extreme statistics regardless of sign.  

\begin{exc}
Do a \Rfunction{geneSetTest} to see if Cell Cycle (KEGG 04110) genes have extreme test-statistics. Are the genes signficantly up-, or down-regulated?
\end{exc}

<<>>=
library(limma)
ccGenes <- unlist(mget("04110", revmap(illuminaHumanv3PATH)))
ccInd <- which(rownames(nsFiltered) %in% ccGenes)
geneSetTest(index = ccInd,statistics=ttests$statistic)
geneSetTest(index = ccInd,statistics=ttests$statistic,alternative="down")
geneSetTest(index = ccInd,statistics=ttests$statistic,alternative="up")

barcodeplot(ttests$statistic,index=ccInd)            
plot(density(ttests$statistic))           
lines(density(ttests$statistic[ccInd]),col="red")
@


\section{Using Bioconductor data packages}

The Bioconductor project has a collection of example datasets. Often these are used as examples to illustrate a particular package or functionality, or to accompany the analysis presented in a publication. For example, several datasets are presented to accompany the \Rpackage{genefu} package which has functions useful for the classification of breast cancer patients based on expression profiles.

An experimental dataset can be installed and loaded as with any other Bioconductor package. The data itself is saved as an object in the package. You will need to see the documentation for the package to find out the relevant object name.
The full list of datasets available through Bioconductor can be found here {\tt http://tinyurl.com/p5scw4a}. 

\begin{exc}
Install the {\tt breastCancerVDX}, {\tt breastCanceTRANSBIG} datasets. What microarray platform were they generated on? How many samples are in each dataset?
\end{exc}

<<>>=
library(breastCancerVDX)
library(breastCancerTRANSBIG)
data(vdx)
data(transbig)

dim(vdx)
dim(transbig)
annotation(vdx)
annotation(transbig)
@


If we want any classifers to be reproducible and applicable to other datasets, it is sensible to exclude probes that do not have sufficient annotation from the analysis. For this, we can use the \Rpackage{genefilter} package as before. The \Rfunction{nsFilter} function performs this annotation-based filtering as well as variance filtering. The output of the function includes details about how many probes were removed at each stage of the filtering.

\begin{exc}
Filter the vdx dataset using to remove low-variances probes, and probes without sufficient annotation. How many probes are removed?
\end{exc}

<<>>=
library(genefilter)
vdx.filt <- nsFilter(vdx)
vdx.filt
vdx.filt <- vdx.filt[[1]]

@


%%\subsection{knn}

%%<<>>=
%%anov <- apply(exprs(vdx), 1, function(x) t.test(x ~ pData(vdx)$er)$p.value)

%%threshold <- quantile(anov, 3000/length(anov))

%5sub.Expression <- exprs(vdx)[which(anov < threshold),]

%%naVals <- apply(sub.Expression , 1,function(x) any(is.na(x)))

%%test <- exprs(mainz)[rownames(sub.Expression),]

%%library(class)

%%res <- knn(train = t(sub.Expression), test = t(sub.Expression),
%%cl = pData(vdx)$er, k = 5)

%%er.ap <- table(res, pData(mainz)$er)
%%er.ap <- 1 - sum(diag(er.ap))/sum(er.ap)
%%@



%%<<>>=
%%library(e1071)
%%data <- t(dat)
%%labels <- erStatus
%%svm.model <- svm(data,labels,type="C-classification",kernel="linear")

%%predicted <- predict(svm.model, data) #
%%sum(predicted != labels)
%%table(true=labels, pred=predicted)

%%svm.cross <- svm(data,labels, type="C-classification",
%%kernel="linear", cross=10)
%%@


We will now attempt to build a classifier using the {\tt pamr} (Predication analysis of Microarrays) \footnote{Not to be confused with the Partition Around Medioids method of clustering!} package.

\begin{exc}
Format the {\tt vdx} data for \Rpackage{pamr}, and train a classifier to predict ER status. For extra clarity in the results, it might be useful to rename the binary er status used in the data package to something more descriptive.
\end{exc}

<<>>=
library(pamr)

dat <- exprs(vdx.filt)
gN <- as.character(fData(vdx.filt)$Gene.symbol)
gI <- featureNames(vdx.filt)
sI <- sampleNames(vdx.filt)

erStatus <- pData(vdx)$er
erStatus <- gsub(0, "ER-", erStatus)
erStatus <- gsub(1, "ER+", erStatus)

train.dat <- list(x = dat, y = erStatus, genenames = gN, geneid = gI,sampleid = sI)
model <- pamr.train(train.dat, n.threshold = 100)
model
@

We can perform cross-validation using the \Rfunction{pamr.cv} function. Printing the output of this function shows a table of how many genes were used at each threshold, and the number of classification errors. Both these values need to be taken into account when choosing a suitable theshold. The \Rfunction{pamr.plotcv} function can assist with this by producing a diagnostic plot which shows how the error changes with the number of genes. In the plot produced by this function there are two panels; the top one shows the errors in the whole dataset and the bottom one considers each class separately. In each panel, the x axis corresponds to the threshold (and number of genes at each threshold) whereas the y-axis is the number of misclassifications.

\begin{exc}
Perform 10-fold cross-validation on the model and plot the results. Why is the maximim number of errors 135?
\end{exc}

<<>>=
model.cv <- pamr.cv(model, train.dat, nfold = 10)
model.cv

pamr.plotcv(model.cv)
@


\textinfo{In the following sections, feel free to experiment with different values of the threshold (which we will call {\tt Delta})}

The misclassifications can easily be visualised as a 'confusion table'. This simply tabulates the classes assigned to each sample against the original label assigned to the sample. e.g. Misclassifications are samples that we thought were 'ER+' but have been assigned to the 'ER-' group by the classifier, or 'ER-' samples assigned as 'ER+' by the classifier.

\begin{exc}
Create a confusion table for your chosen threshold
\end{exc}

<<>>=
Delta <- 8
pamr.confusion(model.cv, Delta)
@

A visual representation of the class separation can be obtained using the \Rfunction{pamr.plotcvprob} function. For each sample there are two circles representing the probabilty of that sample being classified ER- (red) or ER+ (green).
%%<<>>=
%%pamr.plotcen(model, train.dat, Delta)
%%@
\begin{exc}
Plot the cross-validated probabilities for each sample. Which samples seem to be easier to classify; ER positive, or ER negative?
\end{exc}

<<>>=
pamr.plotcvprob(model, train.dat, Delta)
@

There are a couple of ways of extract the details of the genes that have been used in the classifier. We can list their names using the \Rfunction{pamr.listgenes} function, which in our  case these are just returns the microarray probe names. We can however, use these IDs to query the featureData stored with the original {\tt vdx} object. We can also plot the expression values for each gene, coloured according to the class label.


\begin{exc}
Extract the details of the probes used in the classifier and plot each one
\end{exc}

<<>>=
pamr.listgenes(model, train.dat, Delta)
classifierGenes <-pamr.listgenes(model, train.dat, Delta)[,1]
fData(vdx.filt)[classifierGenes,]
pamr.geneplot(model, train.dat, Delta)

@

\noteright{You may get an error message {\tt Error in plot.new(): Figure margins too large} when trying to produce the gene plot. If this occurs, try increasing the size of your plotting window, or decrease the number of genes by decreasing the threshold. Alternatively, the following code will write the plots to a pdf.}

<<>>=
pdf("classifierProfiles.pdf")
for(i in 1:length(classifierGenes)){
  Symbol <- fData(vdx.filt)[classifierGenes[i],"Gene.symbol"]
  boxplot(exprs(vdx.filt)[classifierGenes[i],]~erStatus,main=Symbol)
  }
dev.off()
@

%%<<>>=
%%library(reshape)
%%geneMat <- melt(exprs(vdx.filt)[classifierGenes,order(erStatus)])
%%geneMat <- data.frame(geneMat, ER = erStatus[match(geneMat[,2], sampleNames(vdx))])
%%ggplot(geneMat, aes(x = X2, y = value,col=ER)) + geom_point() + facet_wrap(~X1)
%%@

\begin{exc}
Use the genes identified by the classifier to produce a heatmap to confirm that they separate the samples as expected.
\end{exc}

<<>>=
symbols <- fData(vdx.filt)[classifierGenes,"Gene.symbol"]
heatmap(exprs(vdx.filt)[classifierGenes,],labRow=symbols)
@


\subsection{Testing the model}
We can now test the classifier on an external dataset. We choose the \textit{transbig} dataset for simplicity as it was generated on the same microarray platform

\begin{exc}
Load the TRANSBIG breast cancer dataset and subset to the same genes as the filtered vdx dataset that we have just worked with.
\end{exc}

<<>>=
library(breastCancerTRANSBIG)
data(transbig)
pData(transbig)[1:4,]

transbig.filt <- transbig[featureNames(vdx.filt),]
@

\begin{exc}
Classify each sample in the Transbig dataset using the model generated from the vdx data. How well do the predictions agree with the original labels?
\end{exc}
<<>>=
predClass <- pamr.predict(model, exprs(transbig.filt), Delta)

table(predClass, pData(transbig)$er)

boxplot(pamr.predict(model, exprs(transbig.filt),Delta,type="posterior")[,1]~pData(transbig)$er)

@

\begin{exc}
Make a heatmap of the transbig data using the genes involved in the vxd classifier
\end{exc}
<<>>=
erLab <- as.factor(pData(transbig)$er)
levels(erLab) <- c("blue","yellow")
heatmap(exprs(transbig.filt)[classifierGenes,],labRow=symbols,ColSideColors=as.character(erLab))

@




\section{Optional Extensions}


\subsection{Clustering and classification on the VDX datasets}


\begin{exc}
\textbf{Optional} If you have time, use some of the techniques from earlier in the practical to explore the VDX data
\end{exc}

<<>>=

pr <- prcomp(dist(t(exprs(vdx.filt))))
pcRes <- data.frame(pr$rotation,SampleGroup=pData(vdx)$er,Sample = rownames(pr$x))

ggplot(pcRes, aes(x=PC1,y=PC2,col=as.factor(SampleGroup))) + geom_point()

iqrs <- apply(exprs(vdx.filt),1,IQR)
topVar <- order(iqrs, decreasing=F)[1:50]

erCol <- as.factor(pData(vdx)$er)
levels(erCol) <- c("blue","yellow")
heatmap(exprs(vdx.filt)[topVar,], ColSideColors=as.character(erCol))

@

\subsection{Survival Analysis}

An attractive feature of the {\tt vdx} dataset is that it includes \textit{survival} data for each breast cancer patient. We are not explicitly covering survival analysis in this course, but for your reference, here are the commands to create survival curves when patients are grouped by ER status and tumour grade.


<<>>=
library(survival)
par(mfrow=c(1,2))

plot(survfit(Surv(pData(vdx)$t.dmfs, pData(vdx)$e.dmfs) ~ pData(vdx)$er),col=c("cyan","salmon"))
plot(survfit(Surv(pData(vdx)$t.dmfs, pData(vdx)$e.dmfs) ~ pData(vdx)$grade),col=c("blue","yellow","orange"))

survdiff(Surv(pData(vdx)$t.dmfs, pData(vdx)$e.dmfs) ~ pData(vdx)$er)
survdiff(Surv(pData(vdx)$t.dmfs, pData(vdx)$e.dmfs) ~ pData(vdx)$grade)

@


\end{document}
